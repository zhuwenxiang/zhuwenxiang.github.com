---
layout: post
title: 小样本的胜利
date: 2013-04-11 18:29
comments: true
categories: 统计建模
---

上篇在直觉上解释了随机森林是如何工作的，本文对它的得失与背后原因做一些洞察。欲扬先抑，先说缺点：

1. 预测速度有限
   树方法的集成在训练过程会很快，但当一旦训练结束，给出预测就相对慢了。更精确的模型集成就需要更多的树，这意味着使用模型速度会更慢。虽然在大多数实际情况下不造成什么影响，但当性能对问题很重要时，随机森林就不适用了。

2. 可解释性不强
   随机森林偏重是预测性质的建模工具而不是一个描述工具。如果需要找数据之间关系的解释，随机森林就无能为力了。尤其是相对单决策树模型来说，一堆树的一堆规则，所能提供对数据的洞见实在有限。

3. 对增量不友好
   每增加一条训练样本都需要重新训练，相比贝叶斯方法只需要根据样本调整权重，最近邻方法不用做任何事，随机森林做增量训练的代价太大了。但是这其实也是一个优点，相比SVM或者基于Boosting的算法，随机森林极其容易做成并行的。

有所失必有所得，再说说优点：

1. 很高的准确率
   随机森林可认为是寻找多个窄小领域的专家通过投票产生结果，泛化能力强。实际上随机森林和最近邻方法都可认为是对特征空间的分解。随机森林在训练过程中预先分解好，而最近邻方法根据测试样本再分解特征空间。从这个观点看，随机森林是最近邻方法的一种加速。随机森林在大多数场合都有很好的表现，可以说概括出了事情的本质。
    
2. 适应高维特征大样本
   小样本时随机森林相对经典算法如SVM或Boosting并不占优势，甚至效果更差，这主要由于要么随机性无处施展；大样本时当其他算法基本不work时，随机森林反而有上佳表现。
    
3. 对噪声不敏感
   boosting的算法对噪声非常敏感而很容易过拟合，但随机森林并非基于梯度的，因此不那么敏感。
    
4. 继承树方法的优点
   不管是Y是回归还是分类，不管是X是离散还是连续，随机森林都适用。而且不需数据预处理过程，不需要特征选择，可以判断变量的重要程度，可以度量样本间的相似程度，可以处理不平衡数据。以上其实很多都是基于树的方法所共有的。
    
5. 不需要检验集
   会有一些未被抽中的样本叫做袋外数据(OOB)，可以用来估计泛化误差，而不需用交叉检验来估计。


当然，还可以说一大堆<a href="http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#features">优点</a>但是为什么偏偏随机森林的表现这么好呢，是怎么想出来的呢。我想从根本上，这是属于小样本的胜利。

随机森林，本质上是在小的随机样本上建立了决策树的集合。小的样本集，相比数据集整体具有更弱的表示能力，但同时也让让一些极端的情况更为显著。当寻找对称性时，中心极限定理加大数据集表现完美，但如果寻找一些不对称性质，就非常需要关注小数据集。

《思考·快与慢》这本书里面也提到，盖茨基金会发现小的学校往往比大学校表现更优秀，同时另外一些小的学校也比大学校的表现差得多。也就是说，如果你想找到形成边缘部分的因素，就需要更关注小数据集。

