---
layout: post
title: 如何向外行介绍GBM和RF原理
date: 2015-02-16 15:34
comments: true
categories: 统计建模
---
### GBM: Gradient Boosting Model

想象一下团队的结构，老板在最上面，下面有下属，下属的下面还有下属等等，团队成员就是解释变量(explanatory variable)。假设你有20个团队(trees=20)，每个团队有5名成员(depth=5)，那么你总共有100名成员。现在我们给一本书，让他们读完回答20个问题（训练数据的观察样本数）。假设问题只有二进制的答案："是"或"否"。现在开始这个训练过程，目标是通过建立20个团队（每个团队有5名成员）来获得最可能正确的答案。成员可以同时属于多个团队，成员即使只属于同一个团时也可以有多种角色（角色最多的成员就是我们模型中最重要的解释变量）。

1. 整个过程从一个随机猜测的答案开始，之后计算误差（=实际答案-预测答案）；
2. 建立一个有5名成员的团队，最大限度的减小误差，再计算其误差；
3. 建立第二个有5名成员的团队，进一步减小误差。不过下个团队并不充分信任上个团队的结论，而是假设其答案以概率p是正确的(learning rate)；
4. ...
5. 持续这一过程，直到20个团队都组建完成。

在这个过程中，我们必须决定，建多少个团队(trees)，每个团队有多少名成员(depth)，以及信任上个团队答案的可能性(learning rate)，以使得最终误差是被最小化的。整个训练过程是通过试验误差的方法完成的。

### RF: Random Forest

回想一下《贫民窟的百万富翁》这部电影。

注意到"打给朋友"得到正确答案的可能性一般是小于"现场调查"的，这也解释了RF背后的理论基础：观众调查，是将弱分类器答案取平均（观众作为弱分类器），就能够取得强于单个分类器"打给朋友"的结果，而后者往往被认为是强分类器。

在RF中，每个团队是独立的，我们已经决定了有多少个团队，每名团队只试图回答问题的的一个子集，然后推广他的学习成果到所有的问题上。同时，不是所有成员都会在组建团队时被加入（即成员加入也是随机抽样的）。最后我们把所有团队的答案取平均来得到最终答案。

*注意RF只能用于二分类问题，而GBM可以用于回答多分类问题。*

以上内容翻译自<a href="http://www.quora.com/What-is-Gradient-Boosting-Models-and-Random-Forests-using-layman-terms/answer/Vinod-Gattani">Quora</a>, 有改动。
